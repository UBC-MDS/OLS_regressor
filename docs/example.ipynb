{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Example usage\n",
                "\n",
                "To use `ols_regressor` in a project:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "from ols_regressor.regressor import LinearRegressor\n",
                "from ols_regressor.cross_validate import cross_validate\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "from sklearn.preprocessing import OneHotEncoder, StandardScaler, OrdinalEncoder\n",
                "from sklearn.compose import make_column_transformer\n",
                "from sklearn.model_selection import train_test_split"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "In this instructional guide, we aim to demonstrate the practical application of our `OLS_Regressor` tool through an in-depth analysis of a comprehensive vehicle dataset gathered in Australia for the year 2023. This rich dataset encapsulates a wide range of information, including the latest car prices in Australia, encompassing a diverse array of brands, models, vehicle types, and distinctive features prevalent in the Australian automotive market.\n",
                "\n",
                "The core focus of our study is to accurately predict the market price of vehicles using the basic information provided in the dataset. To achieve this, we employ our `OLS_Regressor` package. As the name indicates, this package is adept at fitting a linear regression model by utilizing the Ordinary Least Squares (OLS) method. It is further equipped with several analytical tools, including methods like `predict` for price estimation and `score` for evaluating model performance. In addition to these features, we have innovatively designed a bespoke `cross_validate` method, specifically tailored for the hyperparameter tuning process to enhance the model's accuracy and efficiency.\n",
                "\n",
                "Moreover, our package has been carefully crafted to align with the design patterns and methodologies used in the renowned `scikit-learn` package, with some minor yet significant modifications. This alignment ensures familiarity for users experienced with `scikit-learn`, while our enhancements offer additional value and unique capabilities."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Data Preprocessing\n",
                "\n",
                "We have already worked on the data for an initial preprocessing which mainly consists of dropping some useless columns. The details of the initial preprocessing can be found in the `data` folder in the Github repository. \n",
                "\n",
                "The initially preprocessed data is stored in the `data/preprocessed_data.csv` file. We read it using `read_csv` method provided by `pandas`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "Index(['Brand', 'UsedOrNew', 'Transmission', 'DriveType', 'FuelType',\n",
                            "       'BodyType', 'Doors', 'Seats', 'Engine_cylinder_number',\n",
                            "       'Engine_total_volume', 'ExteriorColour', 'Year', 'Kilometres', 'Price',\n",
                            "       'fuel_comsumption_liter', 'fuel_comsumption_km'],\n",
                            "      dtype='object')"
                        ]
                    },
                    "execution_count": 13,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "data = pd.read_csv(\"../data/preprocessed_data.csv\")\n",
                "data.columns"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The `Price` column is our response in this study. Therefore, we removed it from the data to create `X` and `y` for training purposes.\n",
                "\n",
                "The data are then split into training and test parts using the `train_test_split` method provided by `scikit-learn`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [],
            "source": [
                "X, y = data.drop(columns=[\"Price\"]), data[\"Price\"]\n",
                "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "A noticeable fact of our data is that the columns belong to various types of features. For an OLS model, the input features should only be numeric. The code snippet below is part of a data preprocessing pipeline. It demonstrates the use of different transformers for various types of features: categorical, ordinal, numeric, and features to be dropped. Let's break down what each part of the code does:\n",
                "\n",
                "1. **Categorical Features**: \n",
                "   - `categorical_features` are those variables that represent categories, such as the brand or type of fuel. These features are transformed using `OneHotEncoder`, which converts categorical data into a format that can be used by machine learning algorithms. The `handle_unknown=\"ignore\"` parameter ensures that if any unknown category is encountered during transformation, it will be ignored rather than throwing an error.\n",
                "\n",
                "2. **Ordinal Features**: \n",
                "   - `ordinal_features` are categorical features but with a clear ordering or ranking (e.g., number of doors in a car). These are transformed using `OrdinalEncoder`. The `handle_unknown=\"use_encoded_value\", unknown_value=999` parameter settings imply that if an unknown category is encountered, it will be assigned a value of 999.\n",
                "\n",
                "3. **Numeric Features**: \n",
                "   - `numeric_features` are continuous numbers (e.g., year of manufacture, kilometers driven). These are standardized using `StandardScaler` to normalize their range and distribution, making them more suitable for many machine learning algorithms.\n",
                "\n",
                "4. **Dropping Features**: \n",
                "   - `drop_features` specifies the features to be excluded from the model. In this case, `fuel_comsumption_liter` is being dropped. The `\"drop\"` transformer is used for this purpose.\n",
                "\n",
                "5. **Column Transformer (`ct`)**:\n",
                "   - `make_column_transformer` is used to apply these transformations to the appropriate columns in the dataset. It creates a single transformer object (`ct`) which applies all the specified transformations to the dataset in a streamlined manner.\n",
                "\n",
                "6. **Transforming the Training Data (`X_train_encoded`)**:\n",
                "   - Finally, `X_train_encoded` is created by applying the `ct` transformer to `X_train`. This results in a transformed training dataset with one-hot encoded, ordinal encoded, standardized, and dropped features, making it ready for use in a machine learning model.\n",
                "\n",
                "This preprocessing step is crucial for preparing the data correctly, ensuring that the machine learning model we choose to apply next can learn effectively from this structured and cleaned data."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "array([[ 0.        ,  0.        ,  0.        , ...,  1.49214463,\n",
                            "         1.41067538,  1.51222474],\n",
                            "       [ 0.        ,  0.        ,  0.        , ..., -0.44649103,\n",
                            "        -0.66911493, -0.71054051],\n",
                            "       [ 0.        ,  0.        ,  0.        , ..., -0.51863016,\n",
                            "        -1.01574664, -1.7538793 ],\n",
                            "       ...,\n",
                            "       [ 0.        ,  0.        ,  0.        , ..., -0.00508252,\n",
                            "        -0.43802711, -0.61981539],\n",
                            "       [ 0.        ,  0.        ,  0.        , ...,  0.65793342,\n",
                            "        -0.43802711, -0.84662817],\n",
                            "       [ 0.        ,  0.        ,  0.        , ..., -0.2266645 ,\n",
                            "        -1.24683446,  0.37816084]])"
                        ]
                    },
                    "execution_count": 6,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "# Categorical features should be encoded with OneHotEncoder\n",
                "categorical_features = ['Brand', 'UsedOrNew', 'Transmission', 'DriveType', 'FuelType',\n",
                "       'BodyType', 'ExteriorColour']\n",
                "# Ordinal features should be encoded with OrdinalEncoder\n",
                "ordinal_features = ['Doors', 'Seats', 'Engine_cylinder_number']\n",
                "# Numeric features should be normalized with StandardScaler\n",
                "numeric_features = ['Year', 'Kilometres', 'Engine_total_volume', 'fuel_comsumption_liter']\n",
                "# Since this feature contains only 100 for all observations, we simply drop it\n",
                "drop_features = ['fuel_comsumption_liter']\n",
                "\n",
                "# make up a column transformers based on the feature types\n",
                "ct = make_column_transformer(\n",
                "    (OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False), categorical_features),\n",
                "    (OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=999), ordinal_features),\n",
                "    (StandardScaler(), numeric_features),\n",
                "    (\"drop\", drop_features)\n",
                ")\n",
                "# fit the column transformer on the training data\n",
                "X_train_encoded = ct.fit_transform(X_train)\n",
                "X_train_encoded"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "array([[ 0.        ,  0.        ,  0.        , ..., -1.07234979,\n",
                            "         0.48632413, -0.07546472],\n",
                            "       [ 0.        ,  0.        ,  0.        , ...,  0.61690914,\n",
                            "         0.71741194,  0.06062295],\n",
                            "       [ 0.        ,  0.        ,  0.        , ..., -1.22961588,\n",
                            "        -0.43802711,  0.55961106],\n",
                            "       ...,\n",
                            "       [ 0.        ,  0.        ,  0.        , ..., -1.22920882,\n",
                            "        -1.01574664, -0.34764006],\n",
                            "       [ 0.        ,  0.        ,  0.        , ..., -0.4935322 ,\n",
                            "        -0.43802711, -0.52909028],\n",
                            "       [ 0.        ,  0.        ,  0.        , ..., -0.08272971,\n",
                            "        -0.43802711, -0.34764006]])"
                        ]
                    },
                    "execution_count": 7,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "X_test_encoded = ct.transform(X_test)\n",
                "X_test_encoded"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Fitting the model on the training data\n",
                "\n",
                "The `fit` function in the `ols_regressor` package will calculate the coefficients for the linear regression model using the Ordinary Least Squares (OLS) method. It converts the input features and target values into NumPy arrays. The function then augments the feature matrix with an intercept term and computes the model coefficients using the OLS formula. The resulting coefficients are stored in the `self.coef` attribute, representing the weights that minimize the sum of squared differences between the predicted and actual target values.\n",
                "\n",
                "The use of this function is demonstrated below.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "array([ 3.31243420e+04, -3.93475551e+04, -3.24646310e+04,  1.59195142e+05,\n",
                            "       -2.52374164e+04, -2.26803002e+04, -5.99355383e+04,  2.01100254e+05,\n",
                            "       -2.02462372e+04, -5.23111571e+04,  4.22082863e+04, -4.60241526e+04,\n",
                            "       -4.31256021e+04, -2.12256241e+04, -1.41643446e+04, -5.61456854e+04,\n",
                            "        1.46990564e+04, -4.78145775e+04, -1.41355982e+04,  4.71981281e+05,\n",
                            "       -4.21568755e+04, -3.55554893e+04, -5.88606161e+04, -5.23137175e+04,\n",
                            "       -6.34807611e+04, -1.50773697e+04, -5.60401972e+04, -9.49431876e+03,\n",
                            "       -5.27748547e+04, -1.11349964e+03, -4.11487549e+04, -3.26223863e+04,\n",
                            "        1.93720776e+04, -3.69586651e+04, -1.60671589e+04, -4.53015641e+04,\n",
                            "        2.06220654e+05, -3.02795247e+04,  5.11685045e+04, -2.09534816e+04,\n",
                            "       -4.06118740e+04, -3.87486786e+04, -5.02415150e+04,  3.00881145e+05,\n",
                            "       -6.54027568e+03, -2.28377052e+04,  5.18866121e+04, -4.98507563e+04,\n",
                            "       -5.64256853e+04,  4.87344179e+04, -3.78121378e+04,  3.34400983e+05,\n",
                            "       -1.58124468e+04, -3.47243058e+04, -3.79464001e+04, -3.72809070e+04,\n",
                            "       -4.09005693e+04,  7.16243909e+04, -3.74947845e+04, -5.06594719e+04,\n",
                            "        3.13868009e+04, -4.65583712e+04, -7.29321274e+03, -3.92775467e+04,\n",
                            "        2.02678636e+05, -3.65082300e+04, -3.93350065e+04, -3.71545265e+04,\n",
                            "       -5.40726600e+04, -4.03327580e+04, -4.18205681e+04, -4.81874292e+04,\n",
                            "       -2.56369161e+04, -2.94822941e+04, -3.54756142e+04, -3.03336259e+04,\n",
                            "        1.71682695e+04,  1.27992667e+04,  3.15679650e+03,  1.39944692e+04,\n",
                            "        1.91298703e+04,  1.01794390e+04,  1.00936079e+04,  5.58341855e+02,\n",
                            "        1.16575046e+04,  6.35446086e+02, -4.86143321e+03,  2.89130557e+04,\n",
                            "        1.93153546e+03, -1.76582261e+04,  4.83442504e+04, -8.28144290e+03,\n",
                            "       -6.56112112e+03, -8.70227872e+03,  7.63422290e+03,  1.75910942e+03,\n",
                            "        2.00107348e+04, -5.09721910e+03,  5.34617099e+03,  7.21259705e+03,\n",
                            "       -3.23498898e+03, -1.47466834e+03,  8.85509521e+02,  8.28712145e+01,\n",
                            "        1.27848049e+03,  5.43480724e+02,  1.44402244e+03,  1.73957897e+03,\n",
                            "       -1.04399667e+03,  1.51303238e+03, -1.82436390e+04,  2.26900371e+03,\n",
                            "        4.76843875e+03,  1.73619578e+03,  2.60459814e+03,  1.44716379e+04,\n",
                            "        6.14042913e+01,  6.11615801e+03, -7.87241904e+02, -3.60291162e+02,\n",
                            "       -1.09781613e+03,  1.54652243e+04,  3.17809714e+02,  3.28258662e+02,\n",
                            "        2.00271074e+02,  1.51714423e+03,  6.50019241e+03,  6.62422153e+03,\n",
                            "       -9.63099633e+03,  3.04591320e+03,  8.36030092e+01])"
                        ]
                    },
                    "execution_count": 8,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "model = LinearRegressor()\n",
                "model.fit(X_train_encoded, y_train)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "array([ 3.31243420e+04, -3.93475551e+04, -3.24646310e+04,  1.59195142e+05,\n",
                            "       -2.52374164e+04, -2.26803002e+04, -5.99355383e+04,  2.01100254e+05,\n",
                            "       -2.02462372e+04, -5.23111571e+04,  4.22082863e+04, -4.60241526e+04,\n",
                            "       -4.31256021e+04, -2.12256241e+04, -1.41643446e+04, -5.61456854e+04,\n",
                            "        1.46990564e+04, -4.78145775e+04, -1.41355982e+04,  4.71981281e+05,\n",
                            "       -4.21568755e+04, -3.55554893e+04, -5.88606161e+04, -5.23137175e+04,\n",
                            "       -6.34807611e+04, -1.50773697e+04, -5.60401972e+04, -9.49431876e+03,\n",
                            "       -5.27748547e+04, -1.11349964e+03, -4.11487549e+04, -3.26223863e+04,\n",
                            "        1.93720776e+04, -3.69586651e+04, -1.60671589e+04, -4.53015641e+04,\n",
                            "        2.06220654e+05, -3.02795247e+04,  5.11685045e+04, -2.09534816e+04,\n",
                            "       -4.06118740e+04, -3.87486786e+04, -5.02415150e+04,  3.00881145e+05,\n",
                            "       -6.54027568e+03, -2.28377052e+04,  5.18866121e+04, -4.98507563e+04,\n",
                            "       -5.64256853e+04,  4.87344179e+04, -3.78121378e+04,  3.34400983e+05,\n",
                            "       -1.58124468e+04, -3.47243058e+04, -3.79464001e+04, -3.72809070e+04,\n",
                            "       -4.09005693e+04,  7.16243909e+04, -3.74947845e+04, -5.06594719e+04,\n",
                            "        3.13868009e+04, -4.65583712e+04, -7.29321274e+03, -3.92775467e+04,\n",
                            "        2.02678636e+05, -3.65082300e+04, -3.93350065e+04, -3.71545265e+04,\n",
                            "       -5.40726600e+04, -4.03327580e+04, -4.18205681e+04, -4.81874292e+04,\n",
                            "       -2.56369161e+04, -2.94822941e+04, -3.54756142e+04, -3.03336259e+04,\n",
                            "        1.71682695e+04,  1.27992667e+04,  3.15679650e+03,  1.39944692e+04,\n",
                            "        1.91298703e+04,  1.01794390e+04,  1.00936079e+04,  5.58341855e+02,\n",
                            "        1.16575046e+04,  6.35446086e+02, -4.86143321e+03,  2.89130557e+04,\n",
                            "        1.93153546e+03, -1.76582261e+04,  4.83442504e+04, -8.28144290e+03,\n",
                            "       -6.56112112e+03, -8.70227872e+03,  7.63422290e+03,  1.75910942e+03,\n",
                            "        2.00107348e+04, -5.09721910e+03,  5.34617099e+03,  7.21259705e+03,\n",
                            "       -3.23498898e+03, -1.47466834e+03,  8.85509521e+02,  8.28712145e+01,\n",
                            "        1.27848049e+03,  5.43480724e+02,  1.44402244e+03,  1.73957897e+03,\n",
                            "       -1.04399667e+03,  1.51303238e+03, -1.82436390e+04,  2.26900371e+03,\n",
                            "        4.76843875e+03,  1.73619578e+03,  2.60459814e+03,  1.44716379e+04,\n",
                            "        6.14042913e+01,  6.11615801e+03, -7.87241904e+02, -3.60291162e+02,\n",
                            "       -1.09781613e+03,  1.54652243e+04,  3.17809714e+02,  3.28258662e+02,\n",
                            "        2.00271074e+02,  1.51714423e+03,  6.50019241e+03,  6.62422153e+03,\n",
                            "       -9.63099633e+03,  3.04591320e+03,  8.36030092e+01])"
                        ]
                    },
                    "execution_count": 9,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "model.coef"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Cross Validation\n",
                "\n",
                "Sometimes, we may need to get both the train score and validation score for hyperparameter tuning. Therefore, we need cross-validation to get the validation performance while avoiding overfitting. Our implementation of cross-validation is somewhat similar to the implementation of `scikit-learn`. The `cross_validate` function accepts five arguments: \n",
                "\n",
                "- `model`: The model to perform cross-validation\n",
                "- `X`: The predictors of training data. It should be a 2D numpy array\n",
                "- `y`: The response of training data. It should be a 1D numpy array\n",
                "- `cv`: The number of folds used for cross-validation\n",
                "- `random_state`: The random state of the random shuffling in the cross-validation\n",
                "\n",
                "Notice that our `cross_validate` does not require a `return_train_score` argument. The train scores are automatically returned in the cross-validation results."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/html": [
                            "<div>\n",
                            "<style scoped>\n",
                            "    .dataframe tbody tr th:only-of-type {\n",
                            "        vertical-align: middle;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe tbody tr th {\n",
                            "        vertical-align: top;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe thead th {\n",
                            "        text-align: right;\n",
                            "    }\n",
                            "</style>\n",
                            "<table border=\"1\" class=\"dataframe\">\n",
                            "  <thead>\n",
                            "    <tr style=\"text-align: right;\">\n",
                            "      <th></th>\n",
                            "      <th>train_score</th>\n",
                            "      <th>test_score</th>\n",
                            "      <th>fit_time</th>\n",
                            "      <th>score_time</th>\n",
                            "    </tr>\n",
                            "  </thead>\n",
                            "  <tbody>\n",
                            "    <tr>\n",
                            "      <th>0</th>\n",
                            "      <td>0.669816</td>\n",
                            "      <td>0.664145</td>\n",
                            "      <td>0.026347</td>\n",
                            "      <td>0.000695</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>1</th>\n",
                            "      <td>0.688293</td>\n",
                            "      <td>0.537996</td>\n",
                            "      <td>0.007791</td>\n",
                            "      <td>0.001042</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>2</th>\n",
                            "      <td>0.666635</td>\n",
                            "      <td>0.670863</td>\n",
                            "      <td>0.072412</td>\n",
                            "      <td>0.000649</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>3</th>\n",
                            "      <td>0.715619</td>\n",
                            "      <td>0.488649</td>\n",
                            "      <td>0.021645</td>\n",
                            "      <td>0.000488</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>4</th>\n",
                            "      <td>0.665131</td>\n",
                            "      <td>0.687616</td>\n",
                            "      <td>0.045048</td>\n",
                            "      <td>0.000465</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>5</th>\n",
                            "      <td>0.672295</td>\n",
                            "      <td>-19.209700</td>\n",
                            "      <td>0.017041</td>\n",
                            "      <td>0.000089</td>\n",
                            "    </tr>\n",
                            "  </tbody>\n",
                            "</table>\n",
                            "</div>"
                        ],
                        "text/plain": [
                            "   train_score  test_score  fit_time  score_time\n",
                            "0     0.669816    0.664145  0.026347    0.000695\n",
                            "1     0.688293    0.537996  0.007791    0.001042\n",
                            "2     0.666635    0.670863  0.072412    0.000649\n",
                            "3     0.715619    0.488649  0.021645    0.000488\n",
                            "4     0.665131    0.687616  0.045048    0.000465\n",
                            "5     0.672295  -19.209700  0.017041    0.000089"
                        ]
                    },
                    "execution_count": 10,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "cv_results = cross_validate(model, X_train_encoded, y_train.to_numpy(), 5, 42)\n",
                "pd.DataFrame(cv_results)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Predicting with the Fitted Model\n",
                "\n",
                "Now that our regression model has been fitted, it is time to utilize it for making predictions on unseen data. The `predict` function within the `ols_regressor` package has been designed for this purpose. This function expects an array-like matrix X of shape (n_samples, n_features) as input so that we can compute the predicted target values with the coefficients stored in the `self.coef` attribute. The `predict` function will return an array containing the model's predictions based on the provided input features.\n",
                "\n",
                "The use of this function is demonstrated below."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "array([55898.43408674, 75474.05804754, 67526.80375232, ...,\n",
                            "       44614.05367543, 33326.253816  , 23928.94731774])"
                        ]
                    },
                    "execution_count": 11,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "model.predict(X_test_encoded)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Scoring the Fitted Model\n",
                "\n",
                "Here we use the `score` function within the `ols_regressor` package. The function takes in X(n_samples, n_features) and y_pred(n_samples, ) as input and calculates the coefficient of determination for the prediction. "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "0.5705212165277045"
                        ]
                    },
                    "execution_count": 12,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "model.score(X_test_encoded, y_test)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.9.18"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
